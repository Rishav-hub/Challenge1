{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation - 5 BatchNormalization",
      "provenance": [],
      "authorship_tag": "ABX9TyMBA779d3H5Nl1oIJwH/t0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishav-hub/Challenge1/blob/main/Documentation_5_BatchNormalization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRK5OAy6L1A"
      },
      "source": [
        "### Batch Normalization\n",
        "- Although after changing various Activation function and initializing weights in different ways its not guareented that Vanishing and Exploding Gradient problems won't come back. \n",
        "- So, In a 2015 paper, Sergey Ioffe and Christian Szegedy proposed a technique called Batch Normalization (BN).\n",
        "- This involves adding an operation in the model just before or after the activation function of each hidden layers.\n",
        "- Simply zero centering and normalizing each input(), then scaling and shifting the results using two new parameters per layers.\n",
        "- Parameters $\\mu_{B} \\Rightarrow$ Batch- Mean, $\\sigma^{2}_{B}\\Rightarrow$Batch Variance are non - trainable\n",
        "- Parameters $\\gamma \\Rightarrow$ Scaling, $\\beta\\Rightarrow$Shifting are trainable\n",
        "\n",
        "Steps - :\n",
        "\n",
        "- Mini Batch Mean $\\Rightarrow \\mu_{B} = \\frac{1}{M_{B}}\\displaystyle\\sum\\limits_{i=0}^n X^{i}$\n",
        "- Mini - Batch Variance $\\Rightarrow \\sigma^{2}_{B} = \\frac {1}{M_{B}}\\displaystyle\\sum\\limits_{i=0}^n (X^{i} - \\mu_{B})$\n",
        "- Normalizing $X^{i} \\Rightarrow$ $\\hat{X}^{i} = \\frac{X^{i} - \\mu_{B}}{\\sqrt{\\sigma^{2}_{B}} + \\epsilon}$, $\\epsilon \\Rightarrow $ smoothning term $10^{-7}$ to avoid zero division error.\n",
        "- $$Z^{i} = \\gamma\\hat{X} + \\beta$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORod0k2GAUte"
      },
      "source": [
        "### Two types of approach - \n",
        "- Batch Normalization layer before the activation function.\n",
        "- Batch Normalization layer after the activation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARMFtGbaAqv3"
      },
      "source": [
        "#### Batch Normalization layer before the activation function\n",
        "A shifted output that lies within the trainable area. This will give the activation function a better input in his trainable part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HchSA7C_ATuF"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDFhbREQBGpR"
      },
      "source": [
        "minist = tf.keras.datasets.mnist\n",
        "\n",
        "(X_train_full, y_train_full), (X_test, y_test) = minist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDtoMU-dBK27"
      },
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train =  y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QlyGF6fBVOf"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        " tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        " tf.keras.layers.BatchNormalization(),\n",
        " tf.keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\", use_bias=False),\n",
        " tf.keras.layers.BatchNormalization(),\n",
        " tf.keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", use_bias=False),\n",
        " tf.keras.layers.BatchNormalization(),\n",
        " tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2siQYogBEk4d"
      },
      "source": [
        "#### Why did we remove the bias terms in the Dense layers ?\n",
        "Since a Batch Normalization layer includes one offset parameter per input, we can remove the bias term from the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yf-6k4agBXuo",
        "outputId": "2880263a-7672-4020-b55c-8551dd0cdb57"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten (Flatten)            (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 300)               235200    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 300)               1200      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 100)               30000     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 100)               400       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 270,946\n",
            "Trainable params: 268,578\n",
            "Non-trainable params: 2,368\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvw7uNH9BfpF",
        "outputId": "b7aa6e47-276e-42c5-9f07-fe090961869d"
      },
      "source": [
        "784 * 4 # mean, variance, gamma and Beta"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qk9NRvXzCZ4-"
      },
      "source": [
        "Combining all BN layers "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cy8D3VF5CSns",
        "outputId": "b0b7dbfc-4a61-4a0a-86da-6b4db93a737d"
      },
      "source": [
        "784 * 4 + 300*4+ 100*4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4736"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H83qOrwcCfhs",
        "outputId": "40930d4b-2e05-42f9-919f-b3675c735eb9"
      },
      "source": [
        "4736//2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2368"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ3rhHnXCjvs",
        "outputId": "3394b980-43d9-4570-d661-f7495b3c7269"
      },
      "source": [
        "271346 - 2368"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "268978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcwYQTMkCvu2"
      },
      "source": [
        "### Why did we divide the total BN layers by 2 ?\n",
        "This is beacause as we can see from the model.summary() only 268,978 are trainable from total 271,346 parameters, this is because we only make $\\gamma    and  \\beta $as trainable parameter. So, to exclude $\\mu and \\sigma$ we divide it by 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqPnRvT4CqNj",
        "outputId": "2a10641f-0e85-43b1-83be-293b64a26a99"
      },
      "source": [
        "for variable in model.layers[1].variables:\n",
        "  print(variable.name, variable.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch_normalization/gamma:0 True\n",
            "batch_normalization/beta:0 True\n",
            "batch_normalization/moving_mean:0 False\n",
            "batch_normalization/moving_variance:0 False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cg1ThXdjDxsd"
      },
      "source": [
        "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
        "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=1e-3) # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
        "METRICS = [\"accuracy\"]\n",
        "\n",
        "model.compile(loss=LOSS_FUNCTION,\n",
        "              optimizer=OPTIMIZER,\n",
        "              metrics=METRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8O4oKTMD28A",
        "outputId": "1f7a5941-91d4-4c75-bc45-f429795c0a2d"
      },
      "source": [
        "EPOCHS = 10\n",
        "VALIDATION_SET = (X_valid, y_valid)\n",
        "\n",
        "new_history = model.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                    validation_data=VALIDATION_SET,batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 0.8477 - accuracy: 0.7419 - val_loss: 0.4948 - val_accuracy: 0.8578\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4685 - accuracy: 0.8631 - val_loss: 0.3934 - val_accuracy: 0.8844\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3987 - accuracy: 0.8831 - val_loss: 0.3460 - val_accuracy: 0.8990\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3630 - accuracy: 0.8926 - val_loss: 0.3170 - val_accuracy: 0.9084\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3348 - accuracy: 0.9017 - val_loss: 0.2945 - val_accuracy: 0.9154\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3185 - accuracy: 0.9050 - val_loss: 0.2804 - val_accuracy: 0.9202\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3026 - accuracy: 0.9106 - val_loss: 0.2682 - val_accuracy: 0.9248\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2884 - accuracy: 0.9156 - val_loss: 0.2573 - val_accuracy: 0.9268\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2785 - accuracy: 0.9179 - val_loss: 0.2488 - val_accuracy: 0.9288\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2679 - accuracy: 0.9223 - val_loss: 0.2431 - val_accuracy: 0.9298\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VijPqSnPF_zF",
        "outputId": "74945888-c0e5-46ce-9452-c55d8b176d83"
      },
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 43.6155 - accuracy: 0.7975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[43.61552047729492, 0.7975000143051147]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XD-e3yLRERPy"
      },
      "source": [
        "#### Batch Normalization layer after the activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtEw0QsID6vY",
        "outputId": "81681fc2-e313-46b1-f37a-8ed6f6673eab"
      },
      "source": [
        "LAYERS_BN_BIAS_FALSE = [\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(300, use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.Dense(100, use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "]\n",
        "\n",
        "model_2 = tf.keras.models.Sequential(LAYERS_BN_BIAS_FALSE)\n",
        "\n",
        "LOSS_FUNCTION = \"sparse_categorical_crossentropy\" # use => tf.losses.sparse_categorical_crossentropy\n",
        "OPTIMIZER = tf.keras.optimizers.SGD(learning_rate=1e-3) # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
        "METRICS = [\"accuracy\"]\n",
        "\n",
        "model_2.compile(loss=LOSS_FUNCTION,\n",
        "              optimizer=OPTIMIZER,\n",
        "              metrics=METRICS)\n",
        "\n",
        "EPOCHS = 10\n",
        "VALIDATION_SET = (X_valid, y_valid)\n",
        "\n",
        "new_history = model_2.fit(X_train, y_train, epochs=EPOCHS,\n",
        "                    validation_data=VALIDATION_SET,batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 8s 4ms/step - loss: 1.1322 - accuracy: 0.6795 - val_loss: 0.6125 - val_accuracy: 0.8482\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5742 - accuracy: 0.8556 - val_loss: 0.4342 - val_accuracy: 0.8920\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.4559 - accuracy: 0.8808 - val_loss: 0.3584 - val_accuracy: 0.9058\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3949 - accuracy: 0.8948 - val_loss: 0.3153 - val_accuracy: 0.9150\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.3546 - accuracy: 0.9041 - val_loss: 0.2862 - val_accuracy: 0.9220\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3273 - accuracy: 0.9103 - val_loss: 0.2652 - val_accuracy: 0.9246\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3040 - accuracy: 0.9167 - val_loss: 0.2472 - val_accuracy: 0.9320\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2858 - accuracy: 0.9207 - val_loss: 0.2339 - val_accuracy: 0.9364\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2706 - accuracy: 0.9249 - val_loss: 0.2232 - val_accuracy: 0.9376\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 6s 4ms/step - loss: 0.2597 - accuracy: 0.9271 - val_loss: 0.2132 - val_accuracy: 0.9412\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSam1AjZGK-t",
        "outputId": "1c19ba3e-d789-4984-f451-9b092708626c"
      },
      "source": [
        "model_2.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 42.7451 - accuracy: 0.8436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42.745121002197266, 0.8435999751091003]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO-b14QZGlFl"
      },
      "source": [
        "#### Observation\n",
        "Model 1 performed better than model 2 in the test set. So, we can conclude that Batch Normalization layer after the activation function has some advantage over other approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcYJBx7EHAgc"
      },
      "source": [
        "### In test time why not use BN layer ?\n",
        "We may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input’s mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRpql8M3Hdfp"
      },
      "source": [
        "### Advantages - \n",
        "- Faster convergance \n",
        "- Helps to reduce Vanishing and Exploding Gradient Issue.\n",
        "- It is not affected by choice of activation function.\n",
        "- No need of scaling\n",
        "\n",
        "### Disadvantage - \n",
        "- Complexity in the network.\n",
        "- Increase in trainable parameters.\n",
        "- Slow prediction\n",
        "\n",
        "### When to use BN ?\n",
        "- Mostly in CNNs.\n",
        "- Deep layers more than 16 layers \n",
        "- To remove Vanishing and Exploding Gradient Issue in RNNs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBo-4d7vGSB_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}