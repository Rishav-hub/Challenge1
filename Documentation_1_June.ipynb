{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation 1-June",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMaL4iTmf6KXWMH4LY3JJ1/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishav-hub/Challenge1/blob/main/Documentation_1_June.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK8bky5XCwdj"
      },
      "source": [
        "### What is Deep Learning ?\n",
        "The use of complex archietecture in providing soluton to some complex task that when solved using ML techniques would be tedious and time consuming, sometimes not possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBr-ukfRDeut"
      },
      "source": [
        "### Why use Deep Learning ?\n",
        "- It has a upper edge over Machine Learning.\n",
        "- Comparing the performance of ML with increase in Amount of Data with the performance of DL, Deep Learning gives a good performance with increase in Data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfoMUSYsEdN8"
      },
      "source": [
        "### What is Artificial Neural Nretworks (ANNs) ?\n",
        "- It is the foundation of artificial intelligence (AI).\n",
        "- It's a piece of archietecture which is designed to simulate the way human brain analyzes and process information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq39RkicFW2r"
      },
      "source": [
        "### How was the first ANNs ?\n",
        "- The first ANNs were designed using logical gates such as AND, OR, NOR, XOR etc.\n",
        "- Using the combination of Gates these archietecture were made.\n",
        "- It only has binary inputs (0 or 1).\n",
        "- It can be referred from [here](https://www.cs.cmu.edu/~./epxing/Class/10715/reading/McCulloch.and.Pitts.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npZar3w4Gq5h"
      },
      "source": [
        "### The Perceptron\n",
        "- It is a single-layer neural network.\n",
        "- One of the First and simple networks compared to the networks used nowdays.\n",
        "\n",
        "    <a title=\"Chrislb / CC BY-SA (http://creativecommons.org/licenses/by-sa/3.0/)\" href=\"https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png\"><img width=\"1024\" alt=\"ArtificialNeuronModel english\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/60/ArtificialNeuronModel_english.png/1024px-ArtificialNeuronModel_english.png\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXqNnKm5HO93"
      },
      "source": [
        "### How were Perceptron Different from the previous ANNs ?\n",
        "- Like previous archietecture it was not made using Conditional Gates.\n",
        "- Its not just having binary inputs, but the inputs can be various values.\n",
        "- It has an Activation function which can be used to alter the output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "strpVugJIPt8"
      },
      "source": [
        "### Derivation Of Perceptron Rule.\n",
        "According to the above diagram lets having certain input \"$\\mathbf{x}$\" \n",
        "\n",
        "Let the inputs be $\\mathbf{x}1$, $\\mathbf{x}2$, $\\mathbf{x}3$\n",
        "\n",
        "corresponding weights \"$\\mathbf{w}$\" and net input $\\mathbf{z} = w_1 x_1 + w_2 x_2 +  w_1 x_3$\n",
        "\n",
        "So in vector form we have \n",
        "\n",
        "$\\begin{matrix}\n",
        "\\mathbf{w} = \\begin{bmatrix}\n",
        "w_1\\\\ \n",
        "\\vdots\\\\ \n",
        "w_n\\end{bmatrix}&&\n",
        "\\mathbf{x} = \\begin{bmatrix}\n",
        "x_1\\\\ \n",
        "\\vdots\\\\ \n",
        "x_n\\end{bmatrix}\\end{matrix}$\n",
        "\n",
        "\n",
        "so  $\\mathbf{z = w^T_1 x_1 +  w^T_2 x_2 +  w^T_3 x_3}$\n",
        "\n",
        "Now these would be passing through the activation function.\n",
        "\n",
        "If X through the actiaation function with an following condition.\n",
        "\n",
        "$\\phi(z) = \\begin{cases}\n",
        "+1 & \\text{ if } z \\geq \\theta \\\\ \n",
        "-1 & \\text{ if } z< \\theta \n",
        "\\end{cases}$\n",
        "\n",
        "Lets simplify the above equation - \n",
        "\n",
        "$\\phi(z) = \\begin{cases}\n",
        "+1 & \\text{ if } z - \\theta \\geq 0\\\\ \n",
        "-1 & \\text{ if } z - \\theta < 0\n",
        "\\end{cases}$\n",
        " \n",
        " Here, $\\mathbf{z} = w_1 x_1 + w_2 x_2 +  w_1 x_3$\n",
        "\n",
        "and Î¸ = $w_0 x_0$\n",
        "\n",
        "$w_0 x_0$ is also known as Bias unit\n",
        "\n",
        "Then, \n",
        "\n",
        "$\\mathbf{z'} = w_0 x_0 + w_1 x_1 + w_2 x_2 + w_1 x_3$\n",
        "\n",
        "and \n",
        "\n",
        "$\\phi(z) = \\begin{cases}\n",
        "+1 & \\text{ if } z \\geq 0\\\\ \n",
        "-1 & \\text{ if } z < 0\n",
        "\\end{cases}$\n",
        "\n",
        "### This completes the perceptron Rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh6p0hQmSrzu"
      },
      "source": [
        "### What are Bias in the equation ?\n",
        "When we compare the equation with the equation of a straight line\n",
        "$\\mathbf{Y} = m x + c$, the bias term is the $c$ term, which is the intercept of the line.\n",
        "\n",
        "Hence, bias $b$ causes shifting of the equation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_ienA_ETtFw"
      },
      "source": [
        "### Why do we need to update weight and biases ?\n",
        "This would help us in in reaching an optimal value of $w$ and $b$ which would give the best prediction with least error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYZrdTWHU7xM"
      },
      "source": [
        "### Perceptron Weight Update Rule.\n",
        "- After we get a predicted value $\\mathbf{y'}$ we need to find the error.\n",
        "- $error = y- y'$\n",
        "- Then the new weights are $w = w + \\triangle w$\n",
        "\n",
        " where , $\\triangle w = \\eta (y- y') \\times x$ , $\\eta = 0 \\rightarrow 1.0  $\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wW-v0EyZQKm"
      },
      "source": [
        "### Gradient Descent understanding.\n",
        "- For every operation it has some or the other cost at which it operates.\n",
        "- The Cost Function in this case would be $(y - y')^2$\n",
        "- Also be written as $f = (y - \\phi(wx + b))^2$ , and $\\phi$ is the activation function\n",
        "#### Image Description\n",
        "![image](https://i.stack.imgur.com/NToQm.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UUyJFH0bXe5"
      },
      "source": [
        "### Working of Gradient Descent. \n",
        "- Let's consider $b =0$ and $y =1$.\n",
        "So the equation will be $f = (1 - w)^2$\n",
        "- If we plot the curve of $f$ with different values of $w$ we would have,\n",
        "![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTmVggekew_lBaJJO5bDb0_tZJcIJf8cP-i8A&usqp=CAU)\n",
        "- Our aim is to reduce or find such a value of $w$ such that we get a minimum $f$.\n",
        "- Lets take an example with $w = 3$\n",
        "\n",
        " $f = (1-3)^2 = 4$\n",
        "\n",
        "- Calculating Gradient - \n",
        "$\\mathbf{f'(w) = -2(1-w)}$\n",
        "\n",
        "   So, $\\mathbf{f'(3) = -2(1-3)} = 4i$\n",
        "- $4i$ is the slope which is pointing towards $+X$, but we need a descent which means we need to subtract this with the current weight to make it near to the optimal value.\n",
        "- $w = w - \\eta \\triangle f(3)$\n",
        "#### Why $\\eta$ should be a smaller value ?\n",
        "If $\\eta = 1$, then $w = 3 - 4 = -1$, this is overshooting the gradient.\n",
        "- We would be continuing updating these value till we get a optimal value for the lowest $f$ or the cost function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCV9LMJRh_5J"
      },
      "source": [
        "### Why do we need Multi Layer Perceptron ?\n",
        "- The solution to fitting more complex (i.e. non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. \n",
        "- The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply add more weights.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm6gr_GWiteP"
      },
      "source": [
        "### How to train a Multi Layer Perceptron ?\n",
        "- moving forward through the network, calculating outputs given inputs and current weight estimates\n",
        "- moving backward updating weights according to the resulting error from forward propagation\n",
        "\n",
        "  ![image](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQIltNSKu69kgHD9Gwh0PjqlNUidNvOMd1DnQ&usqp=CAU)\n",
        "\n",
        "#### So this thing brings the concept of **Back Propagation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPQbg3RDjNY8"
      },
      "source": [
        "### What is Backpropagation in NN ?\n",
        "Lets consider having a single neuron network.\n",
        "- Input layer - $X_1$, $a_0 = x_1$, $w1$\n",
        "- First Layer - $Z_1 = w_1 \\times a_0$, $a_1 = \\sigma(Z_1)$, $w_2$\n",
        "- Second Layer - $Z_2 = w_2 \\times a_2$, $a_2 = \\sigma(Z_2)$, $\\hat{y}$\n",
        "\n",
        "  So the $e = error = (y - \\hat{y})^2$\n",
        "\n",
        "- Updation of $w_2$ $\\Rightarrow$ \n",
        "  $e = (y - a_2)^2$ , $a_2 = \\sigma(z_2)$ , $z_2 = w_2a_1$\n",
        "\n",
        "  $$\\frac{\\partial e}{\\partial w_2} = \\frac{\\partial e}{\\partial a_2} \\times \\frac{\\partial a_2}{\\partial z_2} \\times\\frac{\\partial z_2}{\\partial w_2}$$\n",
        "\n",
        "  So, weight updation after calculating the derivative is,\n",
        "\n",
        "  $$w_2 = w_2 - \\eta \\frac{\\partial e}{\\partial w_2}$$\n",
        "\n",
        "  Similarly, $$w_1 = w_1 - \\eta \\frac{\\partial e}{\\partial w_1}$$\n",
        "\n",
        "#### Similarly using the above method it can also be done using bias terms also"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T602vsdDdtN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}