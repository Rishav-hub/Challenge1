{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Documentation 3- June",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO1A+H1P2JeyzYTjva+wnfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rishav-hub/Challenge1/blob/main/Documentation_3_June.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTjDG48P0xVu"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_RoRRSt2_Qw"
      },
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTzitpMU4mrV"
      },
      "source": [
        "### What are activation functions ?\n",
        "Activation functions helps to determine the output of a neural network. These type of functions are attached to each neuron in the network, and determines whether it should be activated or not, based on whether each neuron’s input is relevant for the model’s prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N29GDl6f2y-B"
      },
      "source": [
        "### Activation Functions\n",
        "\n",
        "#### Sigmoid Activation Function\n",
        "  $\\sigma(x) = \\frac{1}{1+e^{-z}}$\n",
        "  $$where\\ \\sigma(x) \\in (0, 1),\\\\\n",
        "  and\\ x \\in [-\\infty, +\\infty]$$\n",
        "\n",
        "- It's one of the first used activation Functions\n",
        "- Output is in the open interval (0,1).\n",
        "\n",
        "Disadvantages - \n",
        "- Prone to Gradient Vanishing- Due to the saturation region\n",
        "- Output is not zero centered, only outputs zero value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2hYZ4iP6YWU"
      },
      "source": [
        "def update_even_odd_labels(labels):\n",
        "  for idx, label in enumerate(labels):\n",
        "    labels[idx] = np.where(label % 2 == 0, 1, 0)\n",
        "  return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APnor-596Y88"
      },
      "source": [
        "y_train_bin, y_test_bin, y_valid_bin = update_even_odd_labels([y_train, y_test, y_valid])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FT4Iiu_3GzR",
        "outputId": "a4187521-7702-4dd8-c641-86b5bd22f17e"
      },
      "source": [
        "# Implementation\n",
        "model_3 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = [28,28], name = 'input_layer'),\n",
        "                                     tf.keras.layers.Dense(300,activation='relu',name= 'hidden_layer1'),\n",
        "                                     tf.keras.layers.Dense(200, activation = 'relu', name = 'hidden_layer2'),\n",
        "                                     tf.keras.layers.Dense(100, activation = 'relu', name = 'hidden_layer3'),\n",
        "                                     tf.keras.layers.Dense(1, activation='sigmoid', name = 'output_layer')])\n",
        "LOSS_FUNCTION = \"binary_crossentropy\" \n",
        "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
        "METRICS = [\"accuracy\"]\n",
        "\n",
        "model_3.compile(loss=LOSS_FUNCTION,\n",
        "              optimizer=OPTIMIZER, metrics = METRICS)\n",
        "\n",
        "EPOCHS = 10\n",
        "VALIDATION_SET = (X_valid, y_valid_bin)\n",
        "\n",
        "history = model_3.fit(X_train, y_train_bin, epochs=EPOCHS,\n",
        "                    validation_data=VALIDATION_SET,batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 3ms/step - loss: 0.2637 - accuracy: 0.8968 - val_loss: 0.1334 - val_accuracy: 0.9554\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1192 - accuracy: 0.9586 - val_loss: 0.0856 - val_accuracy: 0.9722\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0840 - accuracy: 0.9703 - val_loss: 0.0705 - val_accuracy: 0.9760\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0669 - accuracy: 0.9766 - val_loss: 0.0560 - val_accuracy: 0.9812\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0556 - accuracy: 0.9814 - val_loss: 0.0515 - val_accuracy: 0.9832\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0476 - accuracy: 0.9840 - val_loss: 0.0484 - val_accuracy: 0.9834\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0407 - accuracy: 0.9865 - val_loss: 0.0497 - val_accuracy: 0.9836\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0354 - accuracy: 0.9884 - val_loss: 0.0464 - val_accuracy: 0.9838\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0306 - accuracy: 0.9899 - val_loss: 0.0524 - val_accuracy: 0.9834\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0265 - accuracy: 0.9919 - val_loss: 0.0464 - val_accuracy: 0.9856\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm3ZnNzf4IMY",
        "outputId": "9e30f545-31c8-4d7c-9500-6a7d1b0a8ab3"
      },
      "source": [
        "model_3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_layer (Flatten)        (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "hidden_layer1 (Dense)        (None, 300)               235500    \n",
            "_________________________________________________________________\n",
            "hidden_layer2 (Dense)        (None, 200)               60200     \n",
            "_________________________________________________________________\n",
            "hidden_layer3 (Dense)        (None, 100)               20100     \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 315,901\n",
            "Trainable params: 315,901\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbF00EiC8ROP"
      },
      "source": [
        "#### Hyperbolic tangent activation function\n",
        "\n",
        "The tanh function formula and curve are as follows\n",
        "\n",
        "$$tanh(x) = \\frac{(e^{x} - e^{-x})}{(e^{x} + e^{-x})}$$\n",
        "\n",
        "$$where\\ \\tanh(x) \\in (-1, 1),\\\\\n",
        "and\\ x \\in [-\\infty, +\\infty]$$\n",
        "\n",
        "Advantage over Sigmoid- \n",
        "- It outputs value that range between 1 to -1.\n",
        "\n",
        "It is mostly used in the hidden layers of a binary classification model.\n",
        "\n",
        "**Otherwise the same defects are present in tanh AF as there was in Sigmoid AF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY1LtdKZ4zI3",
        "outputId": "bc0d0c62-4e4d-45ac-c685-e6e15e863222"
      },
      "source": [
        "# Implementation\n",
        "model_1 = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = [28,28], name = 'input_layer'),\n",
        "                                     tf.keras.layers.Dense(300,activation='tanh',name= 'hidden_layer1'),\n",
        "                                     tf.keras.layers.Dense(200, activation = 'tanh', name = 'hidden_layer2'),\n",
        "                                     tf.keras.layers.Dense(100, activation = 'tanh', name = 'hidden_layer3'),\n",
        "                                     tf.keras.layers.Dense(1, activation='sigmoid', name = 'output_layer')])\n",
        "LOSS_FUNCTION = \"binary_crossentropy\" \n",
        "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
        "METRICS = [\"accuracy\"]\n",
        "\n",
        "model_1.compile(loss=LOSS_FUNCTION,\n",
        "              optimizer=OPTIMIZER, metrics = METRICS)\n",
        "\n",
        "EPOCHS = 10\n",
        "VALIDATION_SET = (X_valid, y_valid_bin)\n",
        "\n",
        "history = model_1.fit(X_train, y_train_bin, epochs=EPOCHS,\n",
        "                    validation_data=VALIDATION_SET,batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 2ms/step - loss: 0.2902 - accuracy: 0.8797 - val_loss: 0.2014 - val_accuracy: 0.9224\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1747 - accuracy: 0.9356 - val_loss: 0.1244 - val_accuracy: 0.9574\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1223 - accuracy: 0.9564 - val_loss: 0.0934 - val_accuracy: 0.9686\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 4s 3ms/step - loss: 0.1004 - accuracy: 0.9642 - val_loss: 0.0873 - val_accuracy: 0.9710\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0843 - accuracy: 0.9699 - val_loss: 0.0705 - val_accuracy: 0.9758\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0745 - accuracy: 0.9731 - val_loss: 0.0651 - val_accuracy: 0.9762\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0661 - accuracy: 0.9765 - val_loss: 0.0696 - val_accuracy: 0.9768\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0602 - accuracy: 0.9790 - val_loss: 0.0581 - val_accuracy: 0.9788\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0547 - accuracy: 0.9808 - val_loss: 0.0559 - val_accuracy: 0.9808\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.0508 - val_accuracy: 0.9836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWkfn3UH95pN"
      },
      "source": [
        "#### ReLU Sigmoid Model VS tanh Sigmoid Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeMc0in6-AG_",
        "outputId": "5b4d81d4-af19-4601-a978-6c896b152f48"
      },
      "source": [
        "print(\"Evaluation for ReLU Sigmoid Model is {}\".format(model_3.evaluate(X_test, y_test_bin)))\n",
        "print(\"Evaluation for tanh Sigmoid Model is {}\".format(model_1.evaluate(X_test, y_test_bin)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0492 - accuracy: 0.9846\n",
            "Evaluation for ReLU Sigmoid Model is [0.04918106645345688, 0.9846000075340271]\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0545 - accuracy: 0.9821\n",
            "Evaluation for tanh Sigmoid Model is [0.05450804531574249, 0.9821000099182129]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu3ivxTS-iwL"
      },
      "source": [
        "#### Observation\n",
        "Gradually ReLU Sigmoid model did a slightly better job compared to the other model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of6WYUpS-3p7"
      },
      "source": [
        "### ReLU (Rectified Linear Unit)\n",
        "\n",
        "$$ReLU(x)= max(x,0)$$\n",
        "\n",
        "$$where\\ ReLU(x) \\in (0, x),\\\\\n",
        "and\\ x \\in [-\\infty, +\\infty]$$\n",
        "- The ReLU (Rectified Linear Unit) function is an activation function that is currently more popular. Compared with the sigmod function and the tanh function,\n",
        "\n",
        "Some features are -:\n",
        "- Rectified means, it eliminates all the -ve values.\n",
        "- Calculation is much fater in both direction.\n",
        "- It is not a zero- centeric function.\n",
        "- Suffers from an issue called as **Dying ReLU** - which means that once a negative number is entered, ReLU will die. If you enter a negative number, the gradient will be completely zero, which has the same problem as the sigmod function and tanh function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1Q9xeLt-S9H"
      },
      "source": [
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = pd.DataFrame(housing.target, columns=[\"target\"])\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(X,y, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full,y_train_full, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxhxinrHAHOJ",
        "outputId": "040bf634-d958-452e-8f7f-590618d1e785"
      },
      "source": [
        "LAYERS = [\n",
        "         tf.keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
        "         tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "         tf.keras.layers.Dense(5, activation=\"relu\"),\n",
        "         tf.keras.layers.Dense(1)\n",
        "]\n",
        "\n",
        "model_relu = tf.keras.models.Sequential(LAYERS)\n",
        "\n",
        "LOSS = \"mse\"\n",
        "OPTIMIZER = \"sgd\"\n",
        "\n",
        "model_relu.compile(loss=LOSS , optimizer=OPTIMIZER)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "history = model_relu.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7510 - val_loss: 0.6686\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4427 - val_loss: 0.4028\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4032 - val_loss: 0.4232\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3894 - val_loss: 0.3601\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3789 - val_loss: 0.3690\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3719 - val_loss: 0.3764\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3685 - val_loss: 0.3482\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3604 - val_loss: 0.3423\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3557 - val_loss: 0.3431\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3569 - val_loss: 0.4078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5SgzvkiArQX"
      },
      "source": [
        "### **Leaky ReLU function**\n",
        "\n",
        "$$ \n",
        "leaky\\_relu(x, \\alpha) = \\left\\{\\begin{matrix} \n",
        "x & x\\geq 0 \\\\ \n",
        "\\alpha x & x \\lt 0 \n",
        "\\end{matrix}\\right.\n",
        "$$\n",
        "\n",
        "In order to solve the **Dead ReLU** Problem, people proposed to set the first half of ReLU 0.01x instead of 0.\n",
        "\n",
        "It has a alpha parameter which can be adjusted for setting the slope for the negative part."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaRb--ZuAbCB",
        "outputId": "7e7cd65d-d3dd-4be1-ca9b-227d157d6ef3"
      },
      "source": [
        "LAYERS = [\n",
        "         tf.keras.layers.Dense(30, input_shape=X_train.shape[1:]),\n",
        "         tf.keras.layers.LeakyReLU(),\n",
        "         tf.keras.layers.Dense(10),\n",
        "         tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "         tf.keras.layers.Dense(5),        \n",
        "         tf.keras.layers.LeakyReLU(),\n",
        "\n",
        "         tf.keras.layers.Dense(1)\n",
        "]\n",
        "\n",
        "model_lrelu = tf.keras.models.Sequential(LAYERS)\n",
        "\n",
        "LOSS = \"mse\"\n",
        "OPTIMIZER = \"sgd\"\n",
        "\n",
        "model_lrelu.compile(loss=LOSS , optimizer=OPTIMIZER)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_valid = scaler.transform(X_valid)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "history = model_lrelu.fit(X_train, y_train, epochs=EPOCHS, validation_data=(X_valid, y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.7508 - val_loss: 6.3590\n",
            "Epoch 2/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4530 - val_loss: 2.7119\n",
            "Epoch 3/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.4129 - val_loss: 0.6981\n",
            "Epoch 4/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3854 - val_loss: 0.4066\n",
            "Epoch 5/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3726 - val_loss: 0.3684\n",
            "Epoch 6/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3665 - val_loss: 0.4303\n",
            "Epoch 7/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3634 - val_loss: 0.3409\n",
            "Epoch 8/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3573 - val_loss: 0.4829\n",
            "Epoch 9/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3578 - val_loss: 0.5979\n",
            "Epoch 10/10\n",
            "363/363 [==============================] - 1s 2ms/step - loss: 0.3537 - val_loss: 1.1916\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnKCDzCKBcWR"
      },
      "source": [
        "#### Evaluating ReLU and Leaky ReLU "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHmD2pgvBWmT",
        "outputId": "759c348b-4e7a-4f5c-960d-a3f3567cd87b"
      },
      "source": [
        "print(\"Evaluation for ReLU  Model is {}\".format(model_relu.evaluate(X_test, y_test)))\n",
        "print(\"Evaluation for LeakyReLU Model is {}\".format(model_lrelu.evaluate(X_test, y_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3619\n",
            "Evaluation for ReLU  Model is 0.36190956830978394\n",
            "162/162 [==============================] - 0s 1ms/step - loss: 0.3510\n",
            "Evaluation for LeakyReLU Model is 0.3510229289531708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj1Z7VreB7me"
      },
      "source": [
        "#### Observation\n",
        "We can observe that there is a very little difference between the loss of two models so according to the following dataset both perform well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MMEroIECp_L"
      },
      "source": [
        "### **Softmax activation function**\n",
        "\n",
        "$$S(x_j)=\\frac{e^{x_j}}{\\sum_{k=1}^{K} e^{x_k}}, where\\ j = 1,2, \\cdots, K $$\n",
        "- It also has many applications in Multiclass Classification and neural networks.\n",
        "- Mostly used in the Final Layer\n",
        "- If there are 4 input to the softmax layer then each will have $e^{x_1}, e^{x_2}, e^{x_3}, e^{x_4}$ the it will calculate the probability for individual classes using above equation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_dWLhBfaBr4G",
        "outputId": "1b229de4-2f18-48b5-e326-1778dd4223d2"
      },
      "source": [
        "# Implementation\n",
        "model_soft = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = [28,28], name = 'input_layer'),\n",
        "                                     tf.keras.layers.Dense(300,activation='relu',name= 'hidden_layer1'),\n",
        "                                     tf.keras.layers.Dense(200, activation = 'relu', name = 'hidden_layer2'),\n",
        "                                     tf.keras.layers.Dense(100, activation = 'relu', name = 'hidden_layer3'),\n",
        "                                     tf.keras.layers.Dense(2, activation='softmax', name = 'output_layer')])\n",
        "LOSS_FUNCTION = \"sparse_categorical_crossentropy\"\n",
        "OPTIMIZER = \"SGD\" # or use with custom learning rate=> tf.keras.optimizers.SGD(0.02)\n",
        "METRICS = [\"accuracy\"]\n",
        "\n",
        "model_soft.compile(loss=LOSS_FUNCTION,\n",
        "              optimizer=OPTIMIZER, metrics = METRICS)\n",
        "\n",
        "EPOCHS = 10\n",
        "VALIDATION_SET = (X_valid, y_valid_bin)\n",
        "\n",
        "history = model_soft.fit(X_train, y_train_bin, epochs=EPOCHS,\n",
        "                    validation_data=VALIDATION_SET,batch_size = 32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 [==============================] - 5s 2ms/step - loss: 0.2304 - accuracy: 0.9117 - val_loss: 0.1065 - val_accuracy: 0.9648\n",
            "Epoch 2/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.1014 - accuracy: 0.9648 - val_loss: 0.0712 - val_accuracy: 0.9746\n",
            "Epoch 3/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0734 - accuracy: 0.9750 - val_loss: 0.0659 - val_accuracy: 0.9764\n",
            "Epoch 4/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0591 - accuracy: 0.9800 - val_loss: 0.0517 - val_accuracy: 0.9804\n",
            "Epoch 5/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0493 - accuracy: 0.9834 - val_loss: 0.0555 - val_accuracy: 0.9798\n",
            "Epoch 6/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0424 - accuracy: 0.9857 - val_loss: 0.0485 - val_accuracy: 0.9846\n",
            "Epoch 7/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0360 - accuracy: 0.9881 - val_loss: 0.0435 - val_accuracy: 0.9840\n",
            "Epoch 8/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0307 - accuracy: 0.9902 - val_loss: 0.0573 - val_accuracy: 0.9808\n",
            "Epoch 9/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0264 - accuracy: 0.9920 - val_loss: 0.0427 - val_accuracy: 0.9856\n",
            "Epoch 10/10\n",
            "1719/1719 [==============================] - 4s 2ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.0419 - val_accuracy: 0.9854\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do7Dpg9lEPwq"
      },
      "source": [
        "#### Evaluating both models Sigmoid vs Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3a1C0eYCIbu",
        "outputId": "f658925c-edb6-48f0-b6d0-595b4b82781a"
      },
      "source": [
        "print(\"Evaluation for Sigmoid Model is {}\".format(model_3.evaluate(X_test, y_test_bin)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0452 - accuracy: 0.9832\n",
            "Evaluation for Sigmoid Model is [0.04523968696594238, 0.9832000136375427]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-l-w7DDElLN",
        "outputId": "86e55f37-e9ac-4eae-e300-3c6ecb4679ab"
      },
      "source": [
        "print(\"Evaluation for Softmax Model is {}\".format(model_soft.evaluate(X_test, y_test_bin)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9841\n",
            "Evaluation for Softmax Model is [0.04392661154270172, 0.9840999841690063]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKUxaqSSE3ey"
      },
      "source": [
        "#### Observation\n",
        "Both model performed almost similar to each other. The only difference the way to assign the last layer neuron and the maths behind it obviously."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl-tZS2CE1Rc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}